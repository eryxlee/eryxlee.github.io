<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AGI Explained</title><meta name="author" content="Eryx Lee"><meta name="copyright" content="Eryx Lee"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="AGI Explained">
<meta property="og:url" content="https://blog.agiexplained.com/index.html">
<meta property="og:site_name" content="AGI Explained">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pics.agiexplained.com/i/2024/03/05/171847_2.png">
<meta property="article:author" content="Eryx Lee">
<meta property="article:tag" content="AI,AGI,AIGC,OpenAI,ChatGPT,GPT,Multimodal,LLM,NLP,Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pics.agiexplained.com/i/2024/03/05/171847_2.png"><link rel="shortcut icon" href="https://pics.agiexplained.com/i/2024/03/05/171847.ico"><link rel="canonical" href="https://blog.agiexplained.com/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8823ed47591706858ef7d0072102a1be";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AGI Explained',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-04-02 16:58:28'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pics.agiexplained.com/i/2024/03/05/171847_2.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">62</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://kb.agiexplained.com"><i class="fa-fw fas fa-book"></i><span> 知识库</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://kb.agiexplained.com/#/ai/aiindex"><i class="fa-fw fas fa-book"></i><span> AI资源清单</span></a></li></ul></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="AGI Explained"><span class="site-name">AGI Explained</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://kb.agiexplained.com"><i class="fa-fw fas fa-book"></i><span> 知识库</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://kb.agiexplained.com/#/ai/aiindex"><i class="fa-fw fas fa-book"></i><span> AI资源清单</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/31/vibe-coding/" title="Vibe Coding">Vibe Coding</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-31T11:31:47.000Z" title="发表于 2025-03-31 19:31:47">2025-03-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/General/">General</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Vibe-Coding/">Vibe Coding</a></span></div><div class="content">近期，一个名为 “Vibe Coding”（氛围编程）的概念迅速升温，引发了广泛讨论。它不仅仅是一个时髦术语，更可能预示着软件开发方式的一次深刻变革。
一、 Vibe Coding 的起源“Vibe Coding” 这一术语由 OpenAI 的联合创始人、前特斯拉 AI 负责人 Andrej Karpathy 于 2025 年 2 月首次提出。其核心理念在于：开发者不再需要逐行编写底层代码，而是通过自然语言向 AI（特别是大型语言模型 LLM）描述需求、意图或期望达成的“氛围”（Vibe），由 AI 负责生成相应的代码实现。Karpathy作为AI领域的知名专家，他提出的这一概念迅速引起了开发者社区的广泛关注。Vibe Coding这一术语本身传达了一种”氛围”或”感觉”，象征着一种更加自由、直观的编程方式，开发者可以完全专注于创意和概念，而不必被繁琐的编码细节所困扰。
这标志着一种从指令式编程（告诉计算机“如何做”）向意图驱动编程（告诉计算机“做什么”）的转变。开发者将更多精力聚焦于问题的定义、需求的阐述和最终结果的验证，而非陷入繁琐的语法细节和实现逻辑。
值得注意的是，Vibe C ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/20/Xinference_vs_vllm_vs_ollama/" title="Xinference、Ollama 和 vLLM 比较">Xinference、Ollama 和 vLLM 比较</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-20T13:16:51.000Z" title="发表于 2024-12-20 21:16:51">2024-12-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/LLM/">LLM</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Xinference/">Xinference</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Ollama/">Ollama</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/vLLM/">vLLM</a></span></div><div class="content">以下是 Xinference、Ollama 和 vLLM 三个大模型推理框架的全面对比分析，涵盖功能特性、性能表现、适用场景等核心维度：
一、核心特性对比


特性
Xinference
Ollama
vLLM



开源协议
Apache 2.0
MIT
Apache 2.0


核心定位
企业级多模态推理服务
本地化轻量级LLM运行工具
高吞吐LLM推理框架


模型支持
LLaMA、ChatGLM、Stable Diffusion等
LLaMA、Mistral等GGUF格式模型
HuggingFace格式模型


多模态支持
✅ 文本&#x2F;图像&#x2F;语音
❌ 仅文本
❌ 仅文本


分布式部署
✅ 多节点多GPU
❌ 单机
✅ 单机多GPU


量化支持
✅ 4bit&#x2F;8bit
✅ GGUF量化
✅ AWQ&#x2F;GPTQ


API接口
✅ RESTful&#x2F;gRPC
✅ 简单HTTP接口
✅ RESTful


模型管理
✅ 版本控制&#x2F;缓存
❌ 仅本地文件
❌ 需外部管理


Web UI
✅ 内置管理界面
✅ 第三方工具集成
❌  ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/13/Xinference_local_install/" title="使用Xinference部署本地大模型">使用Xinference部署本地大模型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-13T09:46:18.000Z" title="发表于 2024-12-13 17:46:18">2024-12-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/LLM/">LLM</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Xinference/">Xinference</a></span></div><div class="content">大模型本地化部署已成为企业私有化AI应用的关键需求。本文将基于Xinference（由理才网开源的模型服务框架），结合Docker技术实现本地大模型的快速部署与问答接口开发，支持LLaMA、ChatGLM等主流模型。

一、技术选型对比Xinference核心优势
多模态支持：同时支持文本、图像、语音模型
分布式部署：可横向扩展多GPU&#x2F;多节点
模型管理：内置模型缓存与版本控制
RESTful API：开箱即用的标准化接口


二、环境准备硬件要求
NVIDIA显卡（RTX 3080+&#x2F;显存≥16GB）
磁盘空间：建议预留50GB+（模型存储）

软件依赖1234# 验证环境docker --version              # 需≥20.10nvidia-smi                    # 确认驱动正常docker run --rm --gpus all ubuntu nvidia-smi  # 验证Docker GPU支持


三、构建定制化Docker镜像1. Dockerfile配置123456789101112131415161718 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/07/vllm_local_install/" title="使用vLLM部署本地大模型">使用vLLM部署本地大模型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-07T12:31:38.000Z" title="发表于 2024-12-07 20:31:38">2024-12-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/LLM/">LLM</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/vLLM/">vLLM</a></span></div><div class="content">随着大语言模型（LLM）的普及，如何在本地高效部署和调用模型成为开发者关注的重点。本文将通过Docker容器化技术结合vLLM高性能推理框架，实现本地大模型的快速部署和API服务搭建。
一、技术选型
vLLM  

由伯克利大学开发的高吞吐推理框架
支持HuggingFace模型格式
基于PagedAttention的显存优化技术


Docker  

实现环境隔离与依赖管理
方便模型服务的部署与迁移
支持GPU加速



二、环境准备前置条件
NVIDIA显卡（建议RTX 3090+&#x2F;显存≥24GB）
Docker 20.10+
NVIDIA Container Toolkit
CUDA 11.8+

1234# 验证Docker安装docker --version# 确认NVIDIA支持docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

三、构建Docker镜像1. 创建Dockerfile12345678910111213141516171819FROM nvidia/c ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/01/llama.cpp-build-and-setup/" title="llama.cpp 编译使用">llama.cpp 编译使用</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-01T11:41:21.000Z" title="发表于 2024-12-01 19:41:21">2024-12-01</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/LLM/">LLM</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/ollama/">ollama</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/llama-cpp/">llama.cpp</a></span></div><div class="content">llama.cpp 是一个基于 llama 模型 (https://github.com/ggerganov/llama.cpp) 的 C++ 库，用于在 C++ 程序中运行 LLaMA（Large Language Model Meta AI）模型。
安装必要组件&#x2F;工具
1apt install cmake

编译安装llama.cpp
1234567891011git clone https://github.com/ggerganov/llama.cpp.gitcd llama.cpp# 常规模式构建 llama.cppcmake -B buildcmake --build build --config Release# 使用 Nvidia GPUapt install nvidia-cuda-toolkit -ycmake -B build -DGGML_CUDA=ONcmake --build build --config Release

安装python组件
123pip install sentencepiece# 在llama.cpp目录pip instal ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/11/26/ollama-local-install-with-gpu/" title="使用ollama部署本地大模型（启用GPU）">使用ollama部署本地大模型（启用GPU）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-11-26T06:30:41.000Z" title="发表于 2024-11-26 14:30:41">2024-11-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/LLM/">LLM</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/ollama/">ollama</a></span></div><div class="content">上一篇文章使用Ollama部署了本地大模型，不过那时还没有利用本地的GPU资源。如果需要启用GPU，还需要首先安装nvidia-container-toolkit。
12345678curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpgcurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \    | sed &#x27;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#x27; \    | sudo tee /etc/apt/sources.list.d/nv ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/11/25/ollama-local-install/" title="使用ollama部署本地大模型">使用ollama部署本地大模型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-11-25T04:10:45.000Z" title="发表于 2024-11-25 12:10:45">2024-11-25</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/LLM/">LLM</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/ollama/">ollama</a></span></div><div class="content">Ollama是一个强大的工具，它允许用户在本地环境中运行和定制大型语言模型。而通过Docker技术，可以简化Ollama的安装和部署过程，使得在本地运行如Llama 3这样的开源大型语言模型变得简单快捷。
硬件配置要求由于大模型对硬件配置要求较高，建议使用配置更高的设备，本次测试暂时选用阿里云节点：

CPU：阿里云8vCPU
内存：16GB
显卡：无（使用CPU跑模型）

Docker安装OllamaOllama支持通过Docker安装，这极大地简化了服务器端的部署过程。以下是使用docker compose工具运行Ollama的步骤：

创建 docker-compose.yaml 文件，内容如下：
12345678910version: &#x27;3&#x27;services:    ollama:      image: ollama/ollama:0.4.4      container_name: ollama      ports:        - &quot;11434:11434&quot;      volumes:        - ./data:/root ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/11/21/download-models-from-huggingface/" title="从Huggingface和魔搭下载大模型">从Huggingface和魔搭下载大模型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-11-21T14:13:45.000Z" title="发表于 2024-11-21 22:13:45">2024-11-21</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/LLM/">LLM</a></span></div><div class="content">1、从Huggingface下载安装工具包
1pip install huggingface_hub

设置镜像站点
1export HF_ENDPOINT=&quot;https://hf-mirror.com&quot;

开始下载
12345huggingface-cli download \    --repo-type model \    --local-dir ./Llama-3.2-3B-Chinese \    spxiong/Llama-3.2-3B-Chinese-Instruct

2、从魔搭下载12pip install modelscopepython3 -c &quot;from modelscope import snapshot_download;snapshot_download(&#x27;Qwen/Qwen2.5-3B-Instruct&#x27;, cache_dir=&#x27;./models/&#x27;)&quot;

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/11/15/python-tips-2/" title="python 小技巧整理">python 小技巧整理</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-11-15T11:34:57.000Z" title="发表于 2024-11-15 19:34:57">2024-11-15</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Python/">Python</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Python/">Python</a></span></div><div class="content">1、(:&#x3D;)的使用Python 3.8 中引入，正式名称为赋值表达式，是一种将赋值与表达式相结合的新语法。
123456789101112131415161718# 收集用户输入，直到输入空行lines = []while (line := input(&quot;Enter something (leave blank to quit): &quot;)) != &quot;&quot;:    lines.append(line)    # 筛选生成随机数numbers = [n for _ in range(10) if (n := random.randint(1, 100)) &gt; 50]print(numbers)# 赋值与if判断合成一行a = [1, 2, 3]if (n := len(a)) &gt; 2:    print(f&quot;The list is long enough (&#123;n&#125; elements).&quot;)    # 过滤并打印列表中的长单词words = [&#x27;apple&#x27;, &#x27;b ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/11/11/python-tips-1/" title="python 小技巧整理">python 小技巧整理</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-11-11T03:16:37.000Z" title="发表于 2024-11-11 11:16:37">2024-11-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Python/">Python</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Python/">Python</a></span></div><div class="content">1、下划线(_)的作用
python解析器最后一个表达式的值
一个可以忽略的中间变量，filename, _ = &#39;example.txt&#39;.split(&#39;.&#39;)
格式化一个比较大的数据，amount = 1_000_000

2、zip用法12345678910111213ids = [1, 2, 3]names = [&#x27;Alice&#x27;, &#x27;Bob&#x27;, &#x27;Cathy&#x27;]grades = [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;A+&#x27;]zipped = zip(ids, names, grades)students = list(zipped)   # [(1, &#x27;Alice&#x27;, &#x27;A&#x27;), (2, &#x27;Bob&#x27;, &#x27;B&#x27;), (3, &#x27;Cathy&#x27;, &#x27;A+&#x27;)]unzipped = list(zip(*students))keys = ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/#content-inner">7</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://pics.agiexplained.com/i/2024/03/05/171847_2.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Eryx Lee</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">62</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/eryxlee"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/31/vibe-coding/" title="Vibe Coding">Vibe Coding</a><time datetime="2025-03-31T11:31:47.000Z" title="发表于 2025-03-31 19:31:47">2025-03-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/20/Xinference_vs_vllm_vs_ollama/" title="Xinference、Ollama 和 vLLM 比较">Xinference、Ollama 和 vLLM 比较</a><time datetime="2024-12-20T13:16:51.000Z" title="发表于 2024-12-20 21:16:51">2024-12-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/13/Xinference_local_install/" title="使用Xinference部署本地大模型">使用Xinference部署本地大模型</a><time datetime="2024-12-13T09:46:18.000Z" title="发表于 2024-12-13 17:46:18">2024-12-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/07/vllm_local_install/" title="使用vLLM部署本地大模型">使用vLLM部署本地大模型</a><time datetime="2024-12-07T12:31:38.000Z" title="发表于 2024-12-07 20:31:38">2024-12-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/01/llama.cpp-build-and-setup/" title="llama.cpp 编译使用">llama.cpp 编译使用</a><time datetime="2024-12-01T11:41:21.000Z" title="发表于 2024-12-01 19:41:21">2024-12-01</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/FastAPI/"><span class="card-category-list-name">FastAPI</span><span class="card-category-list-count">7</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/General/"><span class="card-category-list-name">General</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/"><span class="card-category-list-name">LLM</span><span class="card-category-list-count">7</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LangChain/"><span class="card-category-list-name">LangChain</span><span class="card-category-list-count">17</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Prompt/"><span class="card-category-list-name">Prompt</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Pydantic/"><span class="card-category-list-name">Pydantic</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Python/"><span class="card-category-list-name">Python</span><span class="card-category-list-count">21</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/pytest/"><span class="card-category-list-name">pytest</span><span class="card-category-list-count">5</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/SQLModel/" style="font-size: 1.35em; color: #99a3b1">SQLModel</a> <a href="/tags/Vibe-Coding/" style="font-size: 1.1em; color: #999">Vibe Coding</a> <a href="/tags/ollama/" style="font-size: 1.2em; color: #999da3">ollama</a> <a href="/tags/LLM/" style="font-size: 1.3em; color: #99a1ac">LLM</a> <a href="/tags/Ollama/" style="font-size: 1.1em; color: #999">Ollama</a> <a href="/tags/pytest/" style="font-size: 1.25em; color: #999fa7">pytest</a> <a href="/tags/LangChain/" style="font-size: 1.45em; color: #99a7ba">LangChain</a> <a href="/tags/mkdocs/" style="font-size: 1.1em; color: #999">mkdocs</a> <a href="/tags/Docker/" style="font-size: 1.1em; color: #999">Docker</a> <a href="/tags/vLLM/" style="font-size: 1.15em; color: #999b9e">vLLM</a> <a href="/tags/tools/" style="font-size: 1.1em; color: #999">tools</a> <a href="/tags/FastAPI/" style="font-size: 1.4em; color: #99a5b6">FastAPI</a> <a href="/tags/Cache/" style="font-size: 1.2em; color: #999da3">Cache</a> <a href="/tags/Xinference/" style="font-size: 1.15em; color: #999b9e">Xinference</a> <a href="/tags/ruff/" style="font-size: 1.1em; color: #999">ruff</a> <a href="/tags/Pydantic/" style="font-size: 1.1em; color: #999">Pydantic</a> <a href="/tags/Python/" style="font-size: 1.5em; color: #99a9bf">Python</a> <a href="/tags/Celery/" style="font-size: 1.15em; color: #999b9e">Celery</a> <a href="/tags/llama-cpp/" style="font-size: 1.1em; color: #999">llama.cpp</a> <a href="/tags/RAG/" style="font-size: 1.1em; color: #999">RAG</a> <a href="/tags/Prompt/" style="font-size: 1.1em; color: #999">Prompt</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/03/"><span class="card-archive-list-date">三月 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">十一月 2024</span><span class="card-archive-list-count">6</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">九月 2024</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><span class="card-archive-list-count">6</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><span class="card-archive-list-count">5</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">62</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-04-02T08:58:27.409Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Eryx Lee</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a target="_blank" href="https://beian.miit.gov.cn/">浙ICP备2024062350号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>